"""Chat feature routes

Application-specific chat endpoints moved from inference_api to keep
AgentCore Runtime API clean. These endpoints handle:
- Conversation title generation
- Legacy chat streaming
- Multimodal chat input
"""

import asyncio
import json
import logging
from typing import AsyncGenerator

from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.responses import StreamingResponse

from agents.main_agent.session.session_factory import SessionFactory
from apis.app_api.admin.services import get_tool_access_service
from apis.app_api.assistants.services.assistant_service import assistant_exists, get_assistant_with_access_check, mark_share_as_interacted
from apis.app_api.assistants.services.rag_service import augment_prompt_with_context, search_assistant_knowledgebase_with_formatting
from apis.app_api.files.file_resolver import ResolvedFileContent, get_file_resolver
from apis.app_api.sessions.models import SessionMetadata, SessionPreferences
from apis.app_api.sessions.services.messages import get_messages
from apis.app_api.sessions.services.metadata import get_session_metadata, store_session_metadata

# Import models and services from inference_api (shared code)
from apis.inference_api.chat.models import ChatEvent, ChatRequest, FileContent, GenerateTitleRequest, GenerateTitleResponse
from apis.inference_api.chat.routes import stream_conversational_message
from apis.inference_api.chat.service import generate_conversation_title, get_agent
from apis.shared.auth.dependencies import get_current_user
from apis.shared.auth.models import User
from apis.shared.errors import (
    ConversationalErrorEvent,
    ErrorCode,
    StreamErrorEvent,
    build_conversational_error_event,
)
from apis.shared.quota import (
    build_quota_exceeded_event,
    build_quota_warning_event,
    get_quota_checker,
    is_quota_enforcement_enabled,
)

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/chat", tags=["chat"])

# Stream timeout configuration (in seconds)
# Prevents hanging streams and resource exhaustion
STREAM_TIMEOUT_SECONDS = 600  # 10 minutes


@router.post("/generate-title")
async def generate_title(request: GenerateTitleRequest, current_user: User = Depends(get_current_user)):
    """
    Generate a conversation title for a new session.

    This endpoint uses AWS Bedrock Nova Micro to generate a concise,
    descriptive title based on the user's initial message. It's designed
    to be called in parallel with the first chat request.

    The endpoint:
    - Uses JWT authentication to extract user_id
    - Truncates input to ~500 tokens for speed and cost efficiency
    - Calls Nova Micro with temperature=0.3 for consistent output
    - Updates session metadata both locally and in cloud
    - Returns fallback title "New Conversation" on error

    Args:
        request: GenerateTitleRequest with session_id and user input
        current_user: User from JWT token (injected by dependency)

    Returns:
        GenerateTitleResponse with generated title and session_id
    """
    user_id = current_user.user_id
    logger.info(f"Title generation request - Session: {request.session_id}, User: {user_id}")

    try:
        # Generate title using Nova Micro
        title = await generate_conversation_title(session_id=request.session_id, user_id=user_id, user_input=request.input)

        return GenerateTitleResponse(title=title, session_id=request.session_id)

    except Exception as e:
        logger.error(f"Error in generate_title endpoint: {e}")
        # Return fallback instead of raising exception
        # Title generation failures shouldn't break the user experience
        return GenerateTitleResponse(title="New Conversation", session_id=request.session_id)


@router.post("/stream")
async def chat_stream(request: ChatRequest, current_user: User = Depends(get_current_user)):
    """
    Legacy chat stream endpoint (for backward compatibility)
    Uses default tools (all available) if enabled_tools not specified
    Uses the authenticated user's ID from the JWT token.

    Tool authorization:
    - Filters requested tools to only those the user is allowed to use via AppRoles
    - If user has no tool permissions, agent runs without tools

    Quota enforcement (when enabled via ENABLE_QUOTA_ENFORCEMENT=true):
    - Checks user quota before processing
    - Streams quota_exceeded as assistant message if quota exceeded (better UX)
    - Injects quota_warning event into stream if approaching limit
    """
    user_id = current_user.user_id
    logger.info(f"Legacy chat request - Session: {request.session_id}, User: {user_id}, Message: {request.message[:50]}...")

    # Filter tools based on user permissions (RBAC)
    authorized_tools = request.enabled_tools
    try:
        tool_access_service = get_tool_access_service()
        authorized_tools, denied_tools = await tool_access_service.check_access_and_filter(
            user=current_user,
            requested_tools=request.enabled_tools,
            strict=False,  # Don't fail, just filter
        )
        if denied_tools:
            logger.info(f"Filtered out unauthorized tools for user {user_id}: {denied_tools}")
    except Exception as e:
        # Log error but don't block request - fail open for RBAC errors
        logger.error(f"Error filtering tools for user {user_id}: {e}", exc_info=True)

    # Check quota if enforcement is enabled
    quota_warning_event = None
    quota_exceeded_event = None
    if is_quota_enforcement_enabled():
        try:
            quota_checker = get_quota_checker()
            quota_result = await quota_checker.check_quota(user=current_user, session_id=request.session_id)

            if not quota_result.allowed:
                # Quota exceeded - stream as SSE instead of 429 for better UX
                logger.warning(f"Quota exceeded for user {user_id}: {quota_result.message}")
                quota_exceeded_event = build_quota_exceeded_event(quota_result)
            else:
                # Check for warning level
                quota_warning_event = build_quota_warning_event(quota_result)
                if quota_warning_event:
                    logger.info(f"Quota warning for user {user_id}: {quota_result.warning_level}")

        except Exception as e:
            # Log error but don't block request - fail open for quota errors
            logger.error(f"Error checking quota for user {user_id}: {e}", exc_info=True)

    # If quota exceeded, stream the quota exceeded message instead of agent response
    if quota_exceeded_event:
        return StreamingResponse(
            stream_conversational_message(
                message=quota_exceeded_event.message,
                stop_reason="quota_exceeded",
                metadata_event=quota_exceeded_event,
                session_id=request.session_id,
                user_id=user_id,
                user_input=request.message,
            ),
            media_type="text/event-stream",
            headers={"Cache-Control": "no-cache", "X-Accel-Buffering": "no", "X-Session-ID": request.session_id},
        )

    # Handle assistant RAG integration if assistant_id is provided
    assistant = None
    augmented_message = request.message
    system_prompt = None
    context_chunks = []  # RAG context chunks for citation events

    # Get assistant_id from request or session preferences (priority: request > preferences)
    assistant_id_to_use = request.assistant_id
    if not assistant_id_to_use:
        # Check session preferences for persisted assistant
        try:
            existing_metadata = await get_session_metadata(request.session_id, user_id)
            if existing_metadata and existing_metadata.preferences:
                assistant_id_to_use = existing_metadata.preferences.assistant_id
                if assistant_id_to_use:
                    logger.info(f"Using persisted assistant {assistant_id_to_use} from session preferences")
        except Exception as e:
            logger.error(f"Error checking session preferences for assistant: {e}", exc_info=True)
            # Continue without assistant if metadata check fails

    logger.info(f"Chat request received - Session: {request.session_id}, Assistant ID: {assistant_id_to_use}, Message: {request.message[:50]}...")

    if assistant_id_to_use:
        logger.info(f"Assistant RAG requested - Assistant: {assistant_id_to_use}, Session: {request.session_id}")

        # 1. Check if session already has an assistant attached
        # If it does, verify it's the same assistant (can't change assistants mid-session)
        # If it doesn't, verify session has no messages (can only attach to new sessions)
        try:
            existing_metadata = await get_session_metadata(request.session_id, user_id)
            existing_assistant_id = existing_metadata.preferences.assistant_id if existing_metadata and existing_metadata.preferences else None

            if existing_assistant_id:
                # Session already has an assistant - verify it's the same one (if request provided one)
                if request.assistant_id and existing_assistant_id != request.assistant_id:
                    logger.warning(
                        f"Attempted to change assistant from {existing_assistant_id} to {request.assistant_id} in session {request.session_id}"
                    )
                    raise HTTPException(
                        status_code=400, detail="Cannot change assistants mid-session. Start a new session to use a different assistant."
                    )
                # Same assistant or using persisted one - allow it to continue
                logger.info(f"Continuing with existing assistant {assistant_id_to_use} in session {request.session_id}")
            else:
                # No assistant attached - verify session has no messages (can only attach to new sessions)
                # Only check if this is a new attachment (from request, not from preferences)
                if request.assistant_id:
                    messages_response = await get_messages(
                        session_id=request.session_id,
                        user_id=user_id,
                        limit=1,  # Only need to check if any messages exist
                    )
                    if messages_response.messages and len(messages_response.messages) > 0:
                        logger.warning(f"Attempted to attach assistant {request.assistant_id} to session {request.session_id} with existing messages")
                        raise HTTPException(
                            status_code=400, detail="Assistants can only be attached to new sessions, start a new session to chat with this assistant"
                        )
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"Error checking session state: {e}", exc_info=True)
            # Continue anyway - better to allow than block on error

        # 2. Load assistant with access check
        # First check if assistant exists (without access check) to distinguish 404 from 403
        exists = await assistant_exists(assistant_id_to_use)

        if not exists:
            # Assistant doesn't exist
            logger.warning(f"Assistant {assistant_id_to_use} not found for user {user_id}")
            raise HTTPException(status_code=404, detail=f"Assistant not found: {assistant_id_to_use}")

        # Assistant exists, now check access
        assistant = await get_assistant_with_access_check(assistant_id=assistant_id_to_use, user_id=user_id, user_email=current_user.email)

        if not assistant:
            # Assistant exists but access denied (PRIVATE and not owner)
            logger.warning(f"Access denied: user {user_id} attempted to access PRIVATE assistant {assistant_id_to_use}")
            raise HTTPException(status_code=403, detail=f"Access denied: You don't have permission to use this assistant")

        # Mark as viewed if this is a shared assistant (not owned)
        if assistant.owner_id != user_id:
            await mark_share_as_interacted(assistant_id=assistant_id_to_use, user_email=current_user.email)

        # 3. Search assistant knowledge base
        try:
            logger.info(f"Searching knowledge base for assistant {assistant_id_to_use} with query: {request.message[:100]}...")
            context_chunks = await search_assistant_knowledgebase_with_formatting(assistant_id=assistant_id_to_use, query=request.message, top_k=5)
            logger.info(f"Knowledge base search returned {len(context_chunks) if context_chunks else 0} chunks")

            # 4. Augment message with context
            if context_chunks:
                augmented_message = augment_prompt_with_context(user_message=request.message, context_chunks=context_chunks)
                logger.info(
                    f"‚úÖ Augmented message with {len(context_chunks)} context chunks. Original length: {len(request.message)}, Augmented length: {len(augmented_message)}"
                )
            else:
                logger.info(f"‚ö†Ô∏è No context chunks found for assistant {assistant_id_to_use} - using original message without augmentation")
        except Exception as e:
            logger.error(f"‚ùå Error searching assistant knowledge base: {e}", exc_info=True)
            # Continue without RAG context rather than failing

        # 5. Append assistant's instructions to the base system prompt (don't replace)
        if assistant.instructions:
            from agents.main_agent.core.system_prompt_builder import SystemPromptBuilder

            # Build the base prompt with date
            base_prompt_builder = SystemPromptBuilder()
            base_prompt = base_prompt_builder.build(include_date=True)

            # Append assistant instructions to the base prompt
            system_prompt = f"{base_prompt}\n\n## Assistant-Specific Instructions\n\n{assistant.instructions}"
            logger.info(
                f"‚úÖ Appended assistant instructions to base system prompt (base: {len(base_prompt)}, assistant: {len(assistant.instructions)}, total: {len(system_prompt)})"
            )
        else:
            # No assistant instructions - use base prompt if no system_prompt provided
            if not system_prompt:
                from agents.main_agent.core.system_prompt_builder import SystemPromptBuilder

                base_prompt_builder = SystemPromptBuilder()
                system_prompt = base_prompt_builder.build(include_date=True)
            logger.info(f"‚ö†Ô∏è Assistant {assistant_id_to_use} has no instructions - using {'provided' if system_prompt else 'default'} system prompt")

        # 6. Save assistant_id to session preferences (persist for future loads)
        # Only save if it came from the request (not already persisted)
        if request.assistant_id:
            try:
                existing_metadata = await get_session_metadata(request.session_id, user_id)
                if existing_metadata:
                    # Update existing metadata with assistant_id in preferences
                    prefs_dict = existing_metadata.preferences.model_dump(by_alias=False) if existing_metadata.preferences else {}
                    prefs_dict["assistant_id"] = assistant_id_to_use
                    preferences = SessionPreferences(**prefs_dict)

                    updated_metadata = SessionMetadata(
                        session_id=existing_metadata.session_id,
                        user_id=existing_metadata.user_id,
                        title=existing_metadata.title,
                        status=existing_metadata.status,
                        created_at=existing_metadata.created_at,
                        last_message_at=existing_metadata.last_message_at,
                        message_count=existing_metadata.message_count,
                        starred=existing_metadata.starred,
                        tags=existing_metadata.tags,
                        preferences=preferences,
                    )
                else:
                    # Create new metadata with assistant_id in preferences
                    from datetime import datetime, timezone

                    now = datetime.now(timezone.utc).isoformat()
                    preferences = SessionPreferences(assistant_id=assistant_id_to_use)

                    updated_metadata = SessionMetadata(
                        session_id=request.session_id,
                        user_id=user_id,
                        title="New Conversation",
                        status="active",
                        created_at=now,
                        last_message_at=now,
                        message_count=0,  # Will be updated by stream coordinator
                        starred=False,
                        tags=[],
                        preferences=preferences,
                    )

                await store_session_metadata(session_id=request.session_id, user_id=user_id, session_metadata=updated_metadata)
                logger.info(f"üíæ Saved assistant_id {assistant_id_to_use} to session {request.session_id} preferences")
            except Exception as e:
                logger.error(f"Failed to save assistant_id to session preferences: {e}", exc_info=True)
                # Don't fail the request if metadata save fails

    # Resolve file upload IDs to FileContent objects
    files_to_send = list(request.files) if request.files else []

    if request.file_upload_ids:
        logger.info(f"Resolving {len(request.file_upload_ids)} file upload IDs")
        try:
            file_resolver = get_file_resolver()
            resolved_files = await file_resolver.resolve_files(
                user_id=user_id,
                upload_ids=request.file_upload_ids,
                max_files=5,  # Bedrock document limit
            )
            # Convert ResolvedFileContent to FileContent
            for rf in resolved_files:
                files_to_send.append(FileContent(filename=rf.filename, content_type=rf.content_type, bytes=rf.bytes))
            logger.info(f"Resolved {len(resolved_files)} files from upload IDs")
        except Exception as e:
            logger.warning(f"Failed to resolve file upload IDs: {e}")
            # Continue without files rather than failing the request

    try:
        # Get agent instance (with or without tool filtering)
        # Use assistant's system prompt if provided
        agent = get_agent(
            session_id=request.session_id,
            user_id=user_id,
            enabled_tools=authorized_tools,  # Filtered by RBAC (may be None for all allowed)
            system_prompt=system_prompt,  # Assistant instructions if assistant is attached
        )

        # Wrap stream to ensure flush on disconnect and prevent further processing
        async def stream_with_cleanup():
            # Yield quota warning event first if applicable
            if quota_warning_event:
                yield quota_warning_event.to_sse_format()

            # Yield citation events BEFORE the agent stream starts
            # This allows the UI to display sources immediately
            if context_chunks:
                for chunk in context_chunks:
                    citation_event = {
                        "type": "citation",
                        "assistantId": assistant_id_to_use,
                        "documentId": chunk.get("metadata", {}).get("document_id", ""),
                        "fileName": chunk.get("metadata", {}).get("source", "Unknown Source"),
                        "text": chunk.get("text", "")[:500],  # Limit excerpt length
                    }
                    yield f"event: citation\ndata: {json.dumps(citation_event)}\n\n"

            # Pass resolved files (from S3) merged with any direct file content
            # Use augmented message if assistant RAG was applied
            stream_iterator = agent.stream_async(
                augmented_message,  # Use augmented message if assistant RAG was applied
                session_id=request.session_id,
                files=files_to_send if files_to_send else None,
            )

            try:
                # Add timeout to prevent hanging streams
                async with asyncio.timeout(STREAM_TIMEOUT_SECONDS):
                    async for event in stream_iterator:
                        yield event

            except asyncio.TimeoutError:
                # Stream exceeded timeout - send as conversational message
                logger.error(f"‚è±Ô∏è Stream timeout ({STREAM_TIMEOUT_SECONDS}s) for session {request.session_id}")

                # Build conversational timeout error
                timeout_error = Exception(f"Stream processing time exceeded {STREAM_TIMEOUT_SECONDS} seconds")
                error_event = build_conversational_error_event(
                    code=ErrorCode.TIMEOUT, error=timeout_error, session_id=request.session_id, recoverable=True
                )

                # Stream timeout error as assistant message
                yield f'event: message_start\ndata: {{"role": "assistant"}}\n\n'
                yield f'event: content_block_start\ndata: {{"contentBlockIndex": 0, "type": "text"}}\n\n'
                yield f"event: content_block_delta\ndata: {json.dumps({'contentBlockIndex': 0, 'type': 'text', 'text': error_event.message})}\n\n"

                yield f'event: content_block_stop\ndata: {{"contentBlockIndex": 0}}\n\n'
                yield f'event: message_stop\ndata: {{"stopReason": "error"}}\n\n'
                yield error_event.to_sse_format()
                yield "event: done\ndata: {}\n\n"

                # Persist timeout error to session
                try:
                    from strands.types.content import Message
                    from strands.types.session import SessionMessage

                    session_manager = SessionFactory.create_session_manager(session_id=request.session_id, user_id=user_id, caching_enabled=False)

                    user_msg: Message = {"role": "user", "content": [{"text": request.message}]}
                    assistant_msg: Message = {"role": "assistant", "content": [{"text": error_event.message}]}

                    if hasattr(session_manager, "base_manager") and hasattr(session_manager.base_manager, "create_message"):
                        user_session_msg = SessionMessage.from_message(user_msg, 0)
                        assistant_session_msg = SessionMessage.from_message(assistant_msg, 1)
                        session_manager.base_manager.create_message(request.session_id, "default", user_session_msg)
                        session_manager.base_manager.create_message(request.session_id, "default", assistant_session_msg)
                        logger.info(f"üíæ Saved timeout error messages to session {request.session_id}")
                except Exception as persist_error:
                    logger.error(f"Failed to persist timeout error to session: {persist_error}")

                return

            except asyncio.CancelledError:
                # Client disconnected (e.g., stop button clicked)
                logger.warning(f"‚ö†Ô∏è Client disconnected during streaming for session {request.session_id}")

                # Mark session manager as cancelled to prevent further tool execution
                if hasattr(agent.session_manager, "cancelled"):
                    agent.session_manager.cancelled = True
                    logger.info(f"üö´ Session manager marked as cancelled - will ignore further messages")

                # Add final assistant message with stop reason
                stop_message = {"role": "assistant", "content": [{"text": "Session stopped by user"}]}
                if hasattr(agent.session_manager, "pending_messages"):
                    agent.session_manager.pending_messages.append(stop_message)
                    logger.info(f"üìù Added stop message to pending buffer")

                # Re-raise to properly close the connection
                raise

            except Exception as e:
                # Log unexpected errors and send to client as conversational message
                logger.error(f"Error during streaming for session {request.session_id}: {e}", exc_info=True)

                # Build conversational error for better UX
                error_event = build_conversational_error_event(code=ErrorCode.STREAM_ERROR, error=e, session_id=request.session_id, recoverable=True)

                # Stream error as assistant message
                yield f'event: message_start\ndata: {{"role": "assistant"}}\n\n'
                yield f'event: content_block_start\ndata: {{"contentBlockIndex": 0, "type": "text"}}\n\n'
                yield f"event: content_block_delta\ndata: {json.dumps({'contentBlockIndex': 0, 'type': 'text', 'text': error_event.message})}\n\n"

                yield f'event: content_block_stop\ndata: {{"contentBlockIndex": 0}}\n\n'
                yield f'event: message_stop\ndata: {{"stopReason": "error"}}\n\n'
                yield error_event.to_sse_format()
                yield "event: done\ndata: {}\n\n"

                # Persist error messages to session
                try:
                    from strands.types.content import Message
                    from strands.types.session import SessionMessage

                    session_manager = SessionFactory.create_session_manager(session_id=request.session_id, user_id=user_id, caching_enabled=False)

                    user_msg: Message = {"role": "user", "content": [{"text": request.message}]}
                    assistant_msg: Message = {"role": "assistant", "content": [{"text": error_event.message}]}

                    if hasattr(session_manager, "base_manager") and hasattr(session_manager.base_manager, "create_message"):
                        user_session_msg = SessionMessage.from_message(user_msg, 0)
                        assistant_session_msg = SessionMessage.from_message(assistant_msg, 1)
                        session_manager.base_manager.create_message(request.session_id, "default", user_session_msg)
                        session_manager.base_manager.create_message(request.session_id, "default", assistant_session_msg)
                        logger.info(f"üíæ Saved stream error messages to session {request.session_id}")
                except Exception as persist_error:
                    logger.error(f"Failed to persist stream error to session: {persist_error}")

                # Don't re-raise - we've handled the error gracefully
                return

            finally:
                # Cleanup: Flush buffered messages and close stream iterator
                # This runs on both success and error paths
                if hasattr(agent.session_manager, "flush"):
                    try:
                        agent.session_manager.flush()
                        logger.info(f"üíæ Flushed buffered messages for session {request.session_id}")
                    except Exception as flush_error:
                        logger.error(f"Failed to flush session {request.session_id}: {flush_error}")

                # Close the stream iterator if possible
                if hasattr(stream_iterator, "aclose"):
                    try:
                        await stream_iterator.aclose()
                    except Exception as close_error:
                        logger.debug(f"Failed to close stream iterator: {close_error}")

        # Stream response from agent
        return StreamingResponse(
            stream_with_cleanup(),
            media_type="text/event-stream",
            headers={"Cache-Control": "no-cache", "X-Accel-Buffering": "no", "X-Session-ID": request.session_id},
        )

    except HTTPException:
        # Re-raise HTTP exceptions as-is (e.g., from auth)
        raise
    except Exception as e:
        # Stream error as a conversational assistant message for better UX
        logger.error(f"Error in chat_stream: {e}", exc_info=True)

        error_event = build_conversational_error_event(code=ErrorCode.STREAM_ERROR, error=e, session_id=request.session_id, recoverable=True)

        return StreamingResponse(
            stream_conversational_message(
                message=error_event.message,
                stop_reason="error",
                metadata_event=error_event,
                session_id=request.session_id,
                user_id=user_id,
                user_input=request.message,
            ),
            media_type="text/event-stream",
            headers={"Cache-Control": "no-cache", "X-Accel-Buffering": "no", "X-Session-ID": request.session_id},
        )


@router.post("/multimodal")
async def chat_multimodal(request: ChatRequest, current_user: User = Depends(get_current_user)):
    """
    Stream chat response with multimodal input (files)

    For now, just echoes the message and mentions files.
    Will be replaced with actual Strands Agent execution.
    Uses the authenticated user's ID from the JWT token.
    """
    user_id = current_user.user_id
    logger.info(f"Multimodal chat request - Session: {request.session_id}, User: {user_id}")
    logger.info(f"Message: {request.message[:50]}...")
    if request.files:
        logger.info(f"Files: {len(request.files)} uploaded")
        for file in request.files:
            logger.info(f"  - {file.filename} ({file.content_type})")

    async def event_generator():
        try:
            # Send init event
            event = ChatEvent(
                type="init",
                content="Processing multimodal input",
                metadata={"session_id": request.session_id, "file_count": len(request.files or [])},
            )
            yield f"data: {event.to_json()}\n\n"
            await asyncio.sleep(0.2)

            # Echo message
            response_text = f"Received message: '{request.message}'"
            if request.files:
                response_text += f" and {len(request.files)} file(s): "
                response_text += ", ".join([f.filename for f in request.files])

            for word in response_text.split():
                event = ChatEvent(type="text", content=word + " ")
                yield f"data: {event.to_json()}\n\n"
                await asyncio.sleep(0.05)

            # Complete
            event = ChatEvent(type="complete", content="Multimodal processing complete")
            yield f"data: {event.to_json()}\n\n"

        except Exception as e:
            logger.error(f"Error in multimodal event_generator: {e}")
            error_event = ChatEvent(type="error", content=str(e))
            yield f"data: {error_event.to_json()}\n\n"

    return StreamingResponse(event_generator(), media_type="text/event-stream", headers={"Cache-Control": "no-cache", "X-Accel-Buffering": "no"})
