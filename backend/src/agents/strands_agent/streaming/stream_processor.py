"""Stream processor for transforming agent stream events into standardized format.

This module provides functionality to transform raw agent stream events from
Strands Agents into a standardized event format for API consumption.

ARCHITECTURE OVERVIEW:
=====================
The stream processor follows a modular design where:
1. Raw events come from Strands Agents (via agent.stream_async())
2. Each event is processed by specialized handler functions
3. Handlers transform events into a standardized format: {"type": str, "data": dict}
4. Processed events are yielded back to the caller

The main function (process_agent_stream) orchestrates the flow:
- Iterates through raw events from the agent stream
- Calls specialized handlers for different event categories
- Handles errors gracefully
- Ensures all data is JSON-serializable before yielding

ENHANCEMENTS (v2):
==================
DUAL-LAYER ARCHITECTURE:
- Layer 1 (Structural): Content block events provide message reconstruction foundation
  * message_start/stop, content_block_start/delta/stop
  * contentBlockIndex for tracking multiple blocks per message
  * Basic tool use info (toolUseId, name, input)

- Layer 2 (Rich UX): Tool events provide rich user experience enhancements
  * display_content: Formatted code blocks, JSON displays, rich output
  * message: Progress indicators ("Searching...", "Running...")
  * integration metadata: Icons and branding for external tools
  * Execution results (tool_result) and errors (tool_error)

ADDITIONAL ENHANCEMENTS:
- Rich inline citation support with citation_start/citation_end events
- Lifecycle events available for debugging (commented out in production)
- Cache token metrics for prompt caching cost tracking
"""

import logging
from datetime import datetime, date
from decimal import Decimal
from typing import Any, AsyncGenerator, Dict, List, Optional, Tuple, TypedDict
from uuid import UUID

logger = logging.getLogger(__name__)

# Type aliases for better readability
# RawEvent: The raw dictionary structure that comes from Strands Agents
RawEvent = Dict[str, Any]


class ProcessedEventDict(TypedDict):
    """TypedDict for processed events to improve type safety."""
    type: str
    data: Dict[str, Any]


# ProcessedEvent: The standardized format we output: {"type": str, "data": dict}
ProcessedEvent = ProcessedEventDict


def _serialize_object(obj: Any) -> Any:
    """Serialize an object to a JSON-serializable format.

    WHY THIS EXISTS:
    Some objects from Strands (like AgentResult) are not JSON-serializable by default.
    This function recursively converts complex objects into dictionaries/strings
    so they can be safely sent over HTTP/SSE streams.

    HANDLING STRATEGY (in order of preference):
    1. Primitives (str, int, float, bool) - return as-is
    2. Common types (datetime, UUID, Decimal) - convert to string/ISO format
    3. Dictionaries - recursively serialize all values
    4. Lists/tuples - recursively serialize all items
    5. Objects with __dict__ - convert to dict and serialize recursively
    6. Objects with to_dict() method - use that method
    7. Fallback - convert to string representation

    Args:
        obj: Object to serialize (can be any type)

    Returns:
        JSON-serializable representation of the object
    """
    # Handle None - None is JSON-serializable
    if obj is None:
        return None

    # Handle primitive types - these are already JSON-serializable
    if isinstance(obj, (str, int, float, bool)):
        return obj

    # Handle datetime objects - convert to ISO format string
    if isinstance(obj, datetime):
        return obj.isoformat()

    # Handle date objects - convert to ISO format string
    if isinstance(obj, date):
        return obj.isoformat()

    # Handle UUID objects - convert to string
    if isinstance(obj, UUID):
        return str(obj)

    # Handle Decimal objects - convert to string to preserve precision
    # JSON doesn't have a native Decimal type, so we use string representation
    if isinstance(obj, Decimal):
        return str(obj)

    # Handle bytes objects - convert to base64 or string representation
    if isinstance(obj, bytes):
        # For binary data, we could use base64, but for simplicity we'll use repr
        # In production, you might want to use base64.b64encode(obj).decode('utf-8')
        return obj.decode('utf-8', errors='replace')

    # Handle dictionaries - recursively serialize all values
    # This ensures nested objects are also serialized
    if isinstance(obj, dict):
        return {key: _serialize_object(value) for key, value in obj.items()}

    # Handle lists/tuples - recursively serialize all items
    # This handles arrays of complex objects
    if isinstance(obj, (list, tuple)):
        return [_serialize_object(item) for item in obj]

    # Handle dataclasses and objects with __dict__
    # Most Python objects have __dict__ which contains their attributes
    # We convert these to dictionaries and serialize recursively
    if hasattr(obj, "__dict__"):
        return {key: _serialize_object(value) for key, value in obj.__dict__.items()}

    # Handle objects with to_dict method
    # Some objects provide a to_dict() method for serialization
    # We use that method and then serialize the result recursively
    if hasattr(obj, "to_dict"):
        return _serialize_object(obj.to_dict())

    # Fallback: convert to string representation
    # If we can't serialize it any other way, convert to string
    # This ensures we never fail completely, though data may be lost
    try:
        return str(obj)
    except Exception as e:
        logger.warning(
            f"Could not serialize object of type {type(obj)}: {e}, converting to string"
        )
        return str(obj)


def _create_event(event_type: str, data: Dict[str, Any]) -> ProcessedEvent:
    """Create a standardized event dictionary.

    WHY THIS EXISTS:
    All events need to follow the same structure: {"type": str, "data": dict}
    This function ensures consistency and handles serialization automatically.

    Args:
        event_type: The type of event (e.g., "delta", "tool_use", "complete")
        data: Event-specific data dictionary (will be serialized if needed)

    Returns:
        Standardized event dictionary with "type" and "data" keys
        Example: {"type": "delta", "data": {"content": "Hello"}}
    """
    # Serialize data to ensure JSON compatibility
    # This handles cases where data contains non-serializable objects
    serialized_data = _serialize_object(data)
    return {"type": event_type, "data": serialized_data}


def _handle_lifecycle_events(event: RawEvent) -> List[ProcessedEvent]:
    """Process lifecycle events from the agent stream.

    LIFECYCLE EVENTS:
    These events track the state and flow of the agent execution:
    - init_event_loop: Agent is initializing
    - start_event_loop: A new event loop cycle is starting
    - message: A complete message was created
    - event: Raw event from the underlying model stream
    - result: Final result object (contains the complete agent response)

    NOTE: Multiple lifecycle events can be present in a single raw event,
    so we return a list of processed events.

    Args:
        event: Raw event from Strands Agents

    Returns:
        List of processed events (may be empty if no lifecycle events found)
    """
    events = []

    # init_event_loop: True at the start of agent invocation initializing
    # This is the first event when an agent starts processing
    if event.get("init_event_loop", False):
        events.append(_create_event("init_event_loop", {}))

    # start_event_loop: True when the event loop is starting
    # This happens at the beginning of each event loop cycle
    # (an agent may go through multiple cycles if it uses tools)
    if event.get("start_event_loop", False):
        events.append(_create_event("start_event_loop", {}))

    # message: Present when a new message is created
    # IMPORTANT: Message events can occur alongside other events (MessageAddedEvent).
    # We check if it's a valid message structure (has "role" key) to distinguish
    # from cases where "message" appears as part of other event structures.
    if "message" in event:
        message = event["message"]
        # Check if it's a valid message structure (dict with "role" key)
        # This indicates a MessageAddedEvent rather than a nested message reference
        if isinstance(message, dict) and "role" in message:
            # Only exclude if it's clearly part of another event structure
            # (e.g., when it's nested in a result object, not a standalone message event)
            if not any(key in event for key in ["result"]):
                events.append(_create_event("message", {"message": message}))

    # event: Raw event from the model stream
    # This is a pass-through for low-level model events
    if "event" in event:
        events.append(_create_event("event", {"event": event["event"]}))

    # result: The final AgentResult (needs serialization)
    # AgentResult is a complex object that needs to be converted to a dict
    # This contains the final response, metrics, and other completion data
    if "result" in event:
        result_data = _serialize_object(event["result"])
        events.append(_create_event("result", {"result": result_data}))

    return events


def _handle_completion_events(event: RawEvent) -> Tuple[List[ProcessedEvent], bool]:
    """Process completion and error events.

    WHY THIS IS SEPARATE:
    Completion events are special because they signal the end of processing.
    We need to check these FIRST and potentially break out of the loop early.
    That's why this function returns a boolean flag (should_break).

    COMPLETION EVENTS:
    - complete: Normal completion - agent finished successfully
    - force_stop: Error condition - agent was forced to stop

    Args:
        event: Raw event from Strands Agents

    Returns:
        Tuple of (processed events, should_break)
        - processed events: List of completion/error events
        - should_break: True if we should stop processing (complete or error occurred)
    """
    events = []
    should_break = False

    # complete: True when the event loop cycle completes
    # This is the normal completion signal - agent finished successfully
    # We break because there's nothing more to process
    if event.get("complete", False):
        events.append(_create_event("complete", {}))
        should_break = True

    # force_stop: True if the event loop was forced to stop
    # This indicates an error or forced termination
    # We convert this to an "error" event type for consistency
    # We also break because processing should stop on error
    if event.get("force_stop", False):
        reason = event.get("force_stop_reason", "unknown reason")
        events.append(_create_event("error", {"error": f"Agent force-stopped: {reason}"}))
        should_break = True

    return events, should_break


def _handle_content_block_events(event: RawEvent) -> List[ProcessedEvent]:
    """Process content block events from the agent stream.

    CONTENT BLOCK EVENTS - STRUCTURAL FOUNDATION:
    These events provide the structural foundation for reconstructing messages.
    They track the lifecycle and index of each content block (text or tool use).
    This is the core Bedrock Converse API event structure.

    WHY CONTENT BLOCKS ARE THE FOUNDATION:
    - Provides contentBlockIndex for proper message reconstruction
    - Tracks message boundaries (messageStart/Stop)
    - Distinguishes between multiple content blocks in a single message
    - Handles interleaved text and tool use blocks correctly
    - Matches Bedrock Converse API structure directly

    RELATIONSHIP WITH TOOL EVENTS:
    - Content blocks: Structural info (what, where, when in the message)
    - Tool events: Rich UX layer (display_content, progress, execution results)
    - Both are needed for complete rich tool UX experience

    EVENT TYPES (from event.event structure):
    - messageStart: Marks the beginning of a message (contains role)
    - contentBlockStart: Marks the start of a content block (text or tool use)
      * Includes toolUseId, name, type for tool use blocks
      * Includes contentBlockIndex for tracking

    - contentBlockDelta: Incremental updates to a content block
      * Text chunks (for text blocks)
      * Tool input JSON (for tool use blocks)
      * Always includes contentBlockIndex

    - contentBlockStop: Marks the end of a content block
      * Signals completion of a specific block by index

    - messageStop: Marks the end of a message
      * Includes stopReason (end_turn, tool_use, max_tokens, etc.)

    Args:
        event: Raw event from Strands Agents

    Returns:
        List of processed events (may be empty if no content block events found)
    """
    events = []

    # Check if this event has nested event structure
    if "event" not in event or not isinstance(event["event"], dict):
        return events

    inner_event = event["event"]

    # messageStart: Beginning of a message
    # Contains role (typically "assistant")
    if "messageStart" in inner_event:
        message_start = inner_event["messageStart"]
        events.append(_create_event("message_start", {
            "role": message_start.get("role", "assistant")
        }))

    # contentBlockStart: Beginning of a content block
    # Contains start object (toolUse or text) and contentBlockIndex
    if "contentBlockStart" in inner_event:
        block_start = inner_event["contentBlockStart"]
        block_data = {
            "contentBlockIndex": block_start.get("contentBlockIndex", 0)
        }

        # Extract start object (toolUse or text)
        if "start" in block_start:
            start_obj = block_start["start"]

            # Handle tool use start
            if "toolUse" in start_obj:
                tool_use = start_obj["toolUse"]
                block_data["type"] = "tool_use"
                block_data["toolUse"] = {
                    "toolUseId": tool_use.get("toolUseId"),
                    "name": tool_use.get("name"),
                    "type": tool_use.get("type", "tool_use")
                }

            # Handle text start (if model emits it)
            elif "text" in start_obj:
                block_data["type"] = "text"

        events.append(_create_event("content_block_start", block_data))

    # contentBlockDelta: Incremental update to a content block
    # Contains delta object (text or toolUse input) and contentBlockIndex
    if "contentBlockDelta" in inner_event:
        block_delta = inner_event["contentBlockDelta"]
        delta_data = {
            "contentBlockIndex": block_delta.get("contentBlockIndex", 0)
        }

        # Extract delta object
        if "delta" in block_delta:
            delta_obj = block_delta["delta"]

            # Handle text delta
            if "text" in delta_obj:
                delta_data["type"] = "text"
                delta_data["text"] = delta_obj["text"]

            # Handle tool use input delta
            elif "toolUse" in delta_obj:
                tool_use = delta_obj["toolUse"]
                delta_data["type"] = "tool_use"
                if "input" in tool_use:
                    delta_data["input"] = tool_use["input"]

        events.append(_create_event("content_block_delta", delta_data))

    # contentBlockStop: End of a content block
    # Contains contentBlockIndex
    if "contentBlockStop" in inner_event:
        block_stop = inner_event["contentBlockStop"]
        events.append(_create_event("content_block_stop", {
            "contentBlockIndex": block_stop.get("contentBlockIndex", 0)
        }))

    # messageStop: End of a message
    # Contains stopReason (e.g., "end_turn", "tool_use", "max_tokens")
    if "messageStop" in inner_event:
        message_stop = inner_event["messageStop"]
        events.append(_create_event("message_stop", {
            "stopReason": message_stop.get("stopReason", "end_turn")
        }))

    return events


def _extract_tool_use_data(tool_use_data: Dict[str, Any]) -> Dict[str, Any]:
    """Extract and normalize tool use information.

    WHY THIS EXISTS:
    Tool use data comes in different formats from Strands (camelCase vs snake_case).
    This function normalizes it to a consistent format and extracts only the fields
    we need for the API.

    NORMALIZATION:
    - toolUseId / tool_use_id / id -> tool_use_id (standardized)
    - name -> name (always present)
    - input -> input (complete JSON object from Strands)

    ENHANCEMENTS (v2):
    - display_content: Rich display format for UI (code blocks, JSON, etc.)
    - message: Progress message (e.g., "Searching the web", "Running command")
    - context: Additional context for the tool call

    Args:
        tool_use_data: Raw tool use data from event (from "current_tool_use" key)

    Returns:
        Normalized tool use event data with consistent field names
    """
    # Start with the tool name (required field)
    tool_use_event_data = {"name": tool_use_data.get("name")}

    # Add toolUseId if available (useful for tracking multiple tool calls)
    # Handle multiple possible key names for compatibility
    # Strands may use camelCase (toolUseId) or snake_case (tool_use_id) or just "id"
    tool_use_id = (
        tool_use_data.get("toolUseId") or
        tool_use_data.get("tool_use_id") or
        tool_use_data.get("id")
    )
    if tool_use_id:
        tool_use_event_data["tool_use_id"] = tool_use_id

    # Add input if available (complete JSON object from Strands)
    # Since Strands yields complete JSON objects (not partial), this will be complete
    if "input" in tool_use_data:
        tool_use_event_data["input"] = tool_use_data["input"]

    # ENHANCEMENT: Add display_content for richer UI presentation
    # display_content contains formatted representations of tool calls/results
    # Examples:
    # - Code blocks: {"type": "json_block", "json_block": "{\"language\": \"python\", ...}"}
    # - Text blocks: {"type": "text", "text": "Result: ..."}
    if "display_content" in tool_use_data:
        tool_use_event_data["display_content"] = tool_use_data["display_content"]

    # ENHANCEMENT: Add message for progress indicators
    # message provides human-readable status updates
    # Examples: "Searching the web", "Running command", "Creating file"
    if "message" in tool_use_data:
        tool_use_event_data["message"] = tool_use_data["message"]

    # ENHANCEMENT: Add context for additional metadata
    # context may contain additional information about the tool call
    if "context" in tool_use_data:
        tool_use_event_data["context"] = tool_use_data["context"]

    # ENHANCEMENT: Add integration metadata (for external tool integrations)
    # integration_name: Name of the integration (e.g., "google_drive", "slack")
    # integration_icon_url: Icon URL for the integration
    if "integration_name" in tool_use_data:
        tool_use_event_data["integration_name"] = tool_use_data["integration_name"]
    if "integration_icon_url" in tool_use_data:
        tool_use_event_data["integration_icon_url"] = tool_use_data["integration_icon_url"]

    return tool_use_event_data


def _handle_tool_events(event: RawEvent) -> List[ProcessedEvent]:
    """Process tool-related events from the agent stream.

    TOOL EVENTS - RICH UX LAYER:
    These events provide rich user experience enhancements on top of the basic content block
    structure. While content_block events track the structural tool use (what the model wants),
    tool events provide execution results and UX enhancements (how to display it richly).

    WHY BOTH CONTENT BLOCKS AND TOOL EVENTS?
    - Content blocks: Structural foundation (toolUseId, name, input, contentBlockIndex)
    - Tool events: Rich UX layer (display_content, messages, execution results, errors)

    EVENT TYPES:
    - current_tool_use: Strands-enhanced tool call info with UX metadata
      * display_content: Formatted code blocks, JSON displays, rich formatting
      * message: Progress indicators ("Searching the web...", "Running command...")
      * context: Additional metadata for the tool call
      * integration_name/icon_url: External integration branding (Google Drive, Slack, etc.)

    - tool_result / toolResult: Tool execution completed with results
      * Includes display_content for formatted output presentation

    - tool_error: Tool execution failed with error details

    - tool_stream_event: Streaming events from tools themselves (for streaming tools)

    IMPORTANT: Since Strands yields complete JSON objects (not partial), current_tool_use
    events will contain complete tool inputs when emitted.

    ENHANCEMENTS (v2):
    - Extract display_content from tool results for formatted output
    - Extract message field for progress indicators
    - Handle integration metadata for external tools (icons, names)

    Args:
        event: Raw event from Strands Agents

    Returns:
        List of processed events (may be empty if no tool events found)
    """
    events = []

    # current_tool_use: Information about the current tool being used
    # This is emitted when the agent decides to call a tool
    # We check for "name" to ensure it's a valid tool use event
    # Since Strands yields complete JSON objects, the input will be complete
    if "current_tool_use" in event and event["current_tool_use"].get("name"):
        # Normalize the tool use data to a consistent format
        tool_use_event_data = _extract_tool_use_data(event["current_tool_use"])
        events.append(_create_event("tool_use", {"tool_use": tool_use_event_data}))

    # tool_stream_event: Information about an event streamed from a tool
    # Some tools can stream their results incrementally (like sub-agents)
    # This event type handles those streaming tool responses
    if "tool_stream_event" in event:
        events.append(_create_event("tool_stream_event", {
            "tool_stream_event": event["tool_stream_event"]
        }))

    # tool_result / toolResult: Tool execution completed with results
    # Handle both snake_case and camelCase variants for compatibility
    # ENHANCEMENT: Extract display_content from results for better UX
    tool_result = event.get("tool_result") or event.get("toolResult")
    if tool_result:
        result_data = {"tool_result": tool_result}

        # Extract display_content if present (for formatted output)
        if isinstance(tool_result, dict) and "display_content" in tool_result:
            result_data["display_content"] = tool_result["display_content"]

        events.append(_create_event("tool_result", result_data))

    # tool_error: Tool execution failed with an error
    # Handle both snake_case and camelCase variants for compatibility
    tool_error = event.get("tool_error") or event.get("toolError")
    if tool_error:
        events.append(_create_event("tool_error", {"tool_error": tool_error}))

    return events


def _handle_reasoning_events(event: RawEvent) -> List[ProcessedEvent]:
    """Process reasoning-related events from the agent stream.

    REASONING EVENTS:
    Some models (like Claude Sonnet 4.5) support "reasoning" - showing their
    internal thought process before generating the final answer. These events
    capture that reasoning process.

    EVENT TYPES:
    - reasoning: Boolean flag indicating reasoning is happening
    - reasoningText: The actual reasoning text content (top-level or nested)
    - reasoning_signature: Cryptographic signature for reasoning verification
    - redactedContent: Reasoning that was redacted (for privacy/security)
    - reasoningContent: Nested structure containing reasoning data

    NOTE: All reasoning events use the same "reasoning" event type.
    The client can distinguish by checking which keys exist in the data.

    Args:
        event: Raw event from Strands Agents

    Returns:
        List of processed events (may be empty if no reasoning events found)
    """
    events = []

    # reasoning: True for reasoning events
    # This is a boolean flag that reasoning is happening
    # Some models emit this as a separate signal
    if event.get("reasoning", False):
        events.append(_create_event("reasoning", {}))

    # reasoningContent: Nested structure containing reasoning data
    # This is the structured format used by some models
    # It can contain reasoningText, signature, and redactedContent
    if "reasoningContent" in event:
        reasoning_content = event["reasoningContent"]
        reasoning_data = {}

        # Extract nested reasoningText structure
        if isinstance(reasoning_content, dict):
            # Handle reasoningText.text and reasoningText.signature
            if "reasoningText" in reasoning_content:
                reasoning_text_data = reasoning_content["reasoningText"]
                if isinstance(reasoning_text_data, dict):
                    if "text" in reasoning_text_data:
                        reasoning_data["reasoningText"] = reasoning_text_data["text"]
                    if "signature" in reasoning_text_data:
                        reasoning_data["reasoning_signature"] = reasoning_text_data["signature"]
                elif isinstance(reasoning_text_data, str):
                    # Sometimes it's just a string
                    reasoning_data["reasoningText"] = reasoning_text_data

            # Handle redactedContent (binary data)
            if "redactedContent" in reasoning_content:
                reasoning_data["redactedContent"] = reasoning_content["redactedContent"]

            # Handle signature at top level of reasoningContent
            if "signature" in reasoning_content and "reasoning_signature" not in reasoning_data:
                reasoning_data["reasoning_signature"] = reasoning_content["signature"]

        if reasoning_data:
            events.append(_create_event("reasoning", reasoning_data))

    # reasoningText: Text from reasoning process (top-level, for backward compatibility)
    # This contains the actual reasoning content - the model's "thinking"
    # Only process if not already handled via reasoningContent
    if "reasoningText" in event and "reasoningContent" not in event:
        events.append(_create_event("reasoning", {"reasoningText": event["reasoningText"]}))

    # reasoning_signature: Signature from reasoning process (top-level, for backward compatibility)
    # Cryptographic signature that can be used to verify the reasoning wasn't tampered with
    # Only process if not already handled via reasoningContent
    if "reasoning_signature" in event and "reasoningContent" not in event:
        events.append(_create_event("reasoning", {
            "reasoning_signature": event["reasoning_signature"]
        }))

    # redactedContent: Reasoning content redacted by the model (top-level, for backward compatibility)
    # Some reasoning may be redacted for privacy/security reasons
    # This is binary data (bytes) that was redacted
    # Only process if not already handled via reasoningContent
    if "redactedContent" in event and "reasoningContent" not in event:
        events.append(_create_event("reasoning", {
            "redactedContent": event["redactedContent"]
        }))

    return events


def _handle_citation_events(event: RawEvent) -> List[ProcessedEvent]:
    """Process citation events from the agent stream.

    CITATION EVENTS:
    Some models support citations - references to sources or documents that
    support the generated content. These events capture citation information.

    EVENT TYPES:
    - citation: Citation data from models that support citations
    - citationsContent: Array of citations (from content block structure)
    - citation_start: Marks the beginning of cited text (inline citation)
    - citation_end: Marks the end of cited text (inline citation)

    ENHANCEMENTS (v2):
    - Rich citation support with start/stop deltas for inline citations
    - This matches Claude.ai's citation format for RAG/web search integration
    - Clients can highlight cited text and show source information on hover

    INLINE CITATION FLOW:
    1. citation_start emitted with citation metadata (uuid, title, url, sources)
    2. Text content is generated (this text is being cited)
    3. citation_end emitted with citation_uuid to close the citation

    Example UX:
    "According to [OpenAI's documentation](https://openai.com), GPT-4 is..."
                  ^--- citation_start              ^--- citation_end

    Args:
        event: Raw event from Strands Agents

    Returns:
        List of processed events (may be empty if no citation events found)
    """
    events = []

    # ENHANCEMENT: citation_start_delta - marks beginning of cited text
    # This event contains full citation metadata and marks where citation begins
    # The client should track this UUID and associate it with following text
    if "citation_start_delta" in event:
        citation_start = event["citation_start_delta"]

        # Extract citation metadata
        citation_data = {}

        # Handle both direct citation object and nested "citation" key
        citation_obj = citation_start.get("citation", citation_start)

        if isinstance(citation_obj, dict):
            # UUID: Unique identifier for this citation (used to match start/end)
            if "uuid" in citation_obj:
                citation_data["citation_uuid"] = citation_obj["uuid"]

            # Title: Title of the source document/webpage
            if "title" in citation_obj:
                citation_data["title"] = citation_obj["title"]

            # URL: Link to the source
            if "url" in citation_obj:
                citation_data["url"] = citation_obj["url"]

            # Metadata: Additional information about the source
            # (site_domain, favicon_url, site_name, etc.)
            if "metadata" in citation_obj:
                citation_data["metadata"] = citation_obj["metadata"]

            # Origin tool: Which tool generated this citation
            # (e.g., "web_search", "google_drive_search")
            if "origin_tool_name" in citation_obj:
                citation_data["origin_tool_name"] = citation_obj["origin_tool_name"]

            # Sources: Array of source documents that support this citation
            # Each source may have: title, url, content_body, etc.
            if "sources" in citation_obj:
                citation_data["sources"] = citation_obj["sources"]

        if citation_data:
            events.append(_create_event("citation_start", citation_data))

    # ENHANCEMENT: citation_end_delta - marks end of cited text
    # This event contains only the citation UUID to close the citation
    # The client should stop associating text with this citation
    if "citation_end_delta" in event:
        citation_end = event["citation_end_delta"]

        # Extract citation UUID to match with start event
        citation_uuid = citation_end.get("citation_uuid")
        if citation_uuid:
            events.append(_create_event("citation_end", {
                "citation_uuid": citation_uuid
            }))

    # citation: Single citation event (top-level, for backward compatibility)
    # This is emitted when a model includes citation information
    # Only process if not already handled via citation_start_delta
    if "citation" in event and "citation_start_delta" not in event:
        events.append(_create_event("citation", {"citation": event["citation"]}))

    # citationsContent: Array of citations (from content block structure)
    # Some models emit citations as an array within content blocks
    # Only process if not already handled via citation_start_delta
    if "citationsContent" in event and "citation_start_delta" not in event:
        citations = event["citationsContent"]
        # Handle both array and single citation formats
        if isinstance(citations, list):
            for citation in citations:
                events.append(_create_event("citation", {"citation": citation}))
        else:
            events.append(_create_event("citation", {"citation": citations}))

    return events


def _handle_metadata_events(event: RawEvent) -> List[ProcessedEvent]:
    """Extract usage metrics and latency information from metadata events.

    METADATA EVENTS:
    These events contain token usage statistics and performance metrics
    from the model provider. They are typically emitted at the end of
    model invocations or periodically during streaming.

    EVENT TYPES:
    - usage: Token usage information (inputTokens, outputTokens, totalTokens)
    - metrics: Performance metrics (latencyMs, timeToFirstByteMs)
    - metadata: Combined metadata object containing both usage and metrics

    ENHANCEMENTS (v2):
    - Cache token tracking (cacheReadInputTokens, cacheWriteInputTokens)
    - Critical for prompt caching cost analysis and optimization

    NOTE: Metadata events are important for cost tracking and performance monitoring.
    They may appear as separate fields or combined in a metadata object.

    Args:
        event: Raw event from Strands Agents

    Returns:
        List of processed events (may be empty if no metadata events found)
    """
    events = []

    # Check for combined metadata object first
    if "metadata" in event:
        metadata = event["metadata"]
        metadata_data = {}

        # Extract usage information
        if "usage" in metadata:
            usage = metadata["usage"]
            metadata_data["usage"] = {
                "inputTokens": usage.get("inputTokens", 0),
                "outputTokens": usage.get("outputTokens", 0),
                "totalTokens": usage.get("totalTokens", 0),
            }

            # ENHANCEMENT: Add cache token fields for prompt caching metrics
            # These fields track cache hits/writes which are critical for cost optimization
            # Cache reads are 90% cheaper than regular input tokens
            if usage.get("cacheReadInputTokens", 0) > 0:
                metadata_data["usage"]["cacheReadInputTokens"] = usage["cacheReadInputTokens"]
            if usage.get("cacheWriteInputTokens", 0) > 0:
                metadata_data["usage"]["cacheWriteInputTokens"] = usage["cacheWriteInputTokens"]

        # Extract metrics information
        if "metrics" in metadata:
            metrics = metadata["metrics"]
            metadata_data["metrics"] = {
                "latencyMs": metrics.get("latencyMs", 0),
            }
            # Add timeToFirstByteMs if available (streaming-specific metric)
            if "timeToFirstByteMs" in metrics:
                metadata_data["metrics"]["timeToFirstByteMs"] = metrics["timeToFirstByteMs"]

        if metadata_data:
            events.append(_create_event("metadata", metadata_data))

    # Check for standalone usage field (for backward compatibility)
    elif "usage" in event:
        usage = event["usage"]
        usage_data = {
            "inputTokens": usage.get("inputTokens", 0),
            "outputTokens": usage.get("outputTokens", 0),
            "totalTokens": usage.get("totalTokens", 0),
        }

        # Add cache token fields if present
        if usage.get("cacheReadInputTokens", 0) > 0:
            usage_data["cacheReadInputTokens"] = usage["cacheReadInputTokens"]
        if usage.get("cacheWriteInputTokens", 0) > 0:
            usage_data["cacheWriteInputTokens"] = usage["cacheWriteInputTokens"]

        events.append(_create_event("metadata", {
            "usage": usage_data
        }))

    # Check for standalone metrics field (for backward compatibility)
    elif "metrics" in event:
        metrics = event["metrics"]
        metrics_data = {
            "latencyMs": metrics.get("latencyMs", 0),
        }
        if "timeToFirstByteMs" in metrics:
            metrics_data["timeToFirstByteMs"] = metrics["timeToFirstByteMs"]

        events.append(_create_event("metadata", {
            "metrics": metrics_data
        }))

    return events


async def process_agent_stream(
    agent_stream: AsyncGenerator[RawEvent, None]
) -> AsyncGenerator[ProcessedEvent, None]:
    """
    Process and transform raw agent stream events into standardized format.

    MAIN PROCESSING FLOW:
    ====================
    1. Iterate through raw events from agent.stream_async()
    2. For each event, process it through specialized handlers:
       a. Completion events FIRST (may break early)
       b. Lifecycle events (state tracking) - COMMENTED OUT for production, available for debug
       c. Content block events (structural foundation - message/block start/delta/stop)
       d. Tool events (rich UX layer - display_content, progress, execution results)
       e. Reasoning events (model thinking)
       f. Citation events (source references) - ENHANCED with inline citations
       g. Metadata events (usage metrics and performance)
    3. Yield processed events one by one
    4. Send final "done" event when stream completes
    5. Handle errors gracefully
    6. Ensure proper generator cleanup on cancellation

    DUAL-LAYER ARCHITECTURE FOR TOOLS:
    - Content blocks provide structural foundation (contentBlockIndex, basic tool info)
    - Tool events provide rich UX enhancements (display_content, messages, execution results)
    - Both layers work together for complete rich tool experience

    PROCESSING ORDER MATTERS:
    - Completion events are checked FIRST because they signal the end
    - If complete/error occurs, we break immediately (don't process other events)
    - Other events are processed in order but can all occur in the same raw event

    CANCELLATION HANDLING:
    - Client disconnection is detected automatically at the HTTP layer
    - Strands force_stop events are handled via completion events

    EVENT TYPES HANDLED:
    - Lifecycle: init_event_loop, start_event_loop, message, complete, force_stop, result (DEBUG ONLY - commented out)
    - Content Blocks: message_start, content_block_start, content_block_delta, content_block_stop, message_stop
    - Tool: current_tool_use, tool_stream_event, tool_result, tool_error
    - Reasoning: reasoning, reasoningText, reasoning_signature, redactedContent, reasoningContent
    - Citations: citation, citationsContent, citation_start, citation_end (ENHANCED)
    - Metadata: usage, metrics, metadata (token usage with cache tracking, performance metrics)

    ENHANCEMENTS (v2):
    ==================
    - Structured content block tracking (message_start, content_block_start/delta/stop, message_stop)
    - Content block indexing for handling multiple blocks per message
    - Rich inline citation support with citation_start/citation_end events
    - Tool display_content extraction for better UX
    - Tool message/progress indicator support
    - Integration metadata support for external tools
    - Cache token metrics for prompt caching cost analysis

    Args:
        agent_stream: Async generator yielding raw events from Strands Agents

    Yields:
        Dictionary events containing:
        - "type": Event type matching Strands event structure
        - "data": Event-specific data dictionary (always JSON-serializable)

    Example:
        ```python
        agent_stream = agent.stream_async(message)
        async for event in process_agent_stream(agent_stream):
            # event format: {"type": "content_block_delta", "data": {"text": "..."}}
            print(event["type"], event["data"])
        ```

    Testing:
        ```python
        # Mock a simple text stream with citations
        async def mock_stream():
            yield {"start_event_loop": True}
            yield {"event": {"contentBlockDelta": {"delta": {"text": "According to "}}}}
            yield {
                "citation_start_delta": {
                    "citation": {
                        "uuid": "abc-123",
                        "title": "Example Source",
                        "url": "https://example.com"
                    }
                }
            }
            yield {"event": {"contentBlockDelta": {"delta": {"text": "the documentation"}}}}
            yield {"citation_end_delta": {"citation_uuid": "abc-123"}}
            yield {"event": {"contentBlockDelta": {"delta": {"text": ", this is correct."}}}}
            yield {"complete": True}

        events = []
        async for event in process_agent_stream(mock_stream()):
            events.append(event)

        assert events[0]["type"] == "start_event_loop"
        assert events[1]["data"]["text"] == "According to "
        assert events[2]["type"] == "citation_start"
        assert events[2]["data"]["citation_uuid"] == "abc-123"
        ```
    """
    try:
        # Iterate through each raw event from the agent stream
        # The agent stream is an async generator that yields events as they occur
        async for event in agent_stream:

            # Validate event structure (basic check)
            if not isinstance(event, dict):
                logger.warning(f"Unexpected event type: {type(event)}, expected dict")
                continue

            # STEP 1: Process completion/error events FIRST (may break the loop)
            # These events signal the end of processing, so we check them first
            # If we get a complete or error, we should stop processing immediately
            completion_events, should_break = _handle_completion_events(event)
            for processed_event in completion_events:
                yield processed_event
            if should_break:
                # Break out of the loop - processing is complete or errored
                break

            # STEP 2: Process lifecycle events (COMMENTED OUT FOR NOW)
            # NOTE FOR DEVELOPERS: Uncomment the following lines to debug and view all lifecycle events
            # These track the state and flow of agent execution (init_event_loop, start_event_loop, message, result)
            # Multiple lifecycle events can occur in a single raw event
            # for processed_event in _handle_lifecycle_events(event):
            #     yield processed_event

            # STEP 3: Process content block events
            # These track the lifecycle of content blocks (text and tool uses) in the model's response
            # Provides structured tracking with contentBlockIndex, messageStart/Stop, etc.
            for processed_event in _handle_content_block_events(event):
                yield processed_event

            # STEP 4: Process tool events (ENHANCED with display_content)
            # These occur when the agent decides to use a tool
            # Since Strands yields complete JSON objects, inputs will be complete
            for processed_event in _handle_tool_events(event):
                yield processed_event

            # STEP 5: Process reasoning events
            # These show the model's internal thought process (if supported)
            # Not all models support reasoning
            for processed_event in _handle_reasoning_events(event):
                yield processed_event

            # STEP 6: Process citation events (ENHANCED with inline citations)
            # These contain source references from models that support citations
            # Now includes citation_start and citation_end for inline citations
            for processed_event in _handle_citation_events(event):
                yield processed_event

            # STEP 7: Process metadata events (ENHANCED with cache tokens)
            # These contain token usage (including cache metrics) and performance data
            # Important for cost tracking and monitoring
            for processed_event in _handle_metadata_events(event):
                yield processed_event

        # STEP 8: Send final done event
        # This signals to the client that the stream has completed successfully
        # It's sent after all events have been processed
        yield _create_event("done", {})

    except GeneratorExit:
        # Handle generator cleanup when consumer cancels/closes
        # This ensures proper resource cleanup when the client disconnects
        logger.debug("Stream processor generator closed by consumer")
        # GeneratorExit should be re-raised to properly close the generator
        raise

    except StopAsyncIteration:
        # Normal end of iteration - stream completed naturally
        # This is expected and not an error
        logger.debug("Stream ended normally")
        # Send done event if we haven't already
        yield _create_event("done", {})

    except RuntimeError as e:
        # RuntimeError may occur during generator cleanup or cancellation
        # Check if it's related to generator state
        if "generator" in str(e).lower() or "async" in str(e).lower():
            logger.debug(f"Generator runtime error (likely cleanup): {e}")
        else:
            # Unexpected RuntimeError - treat as error
            logger.error(f"Runtime error processing agent stream: {e}", exc_info=True)
            yield _create_event("error", {"error": str(e)})

    except Exception as e:
        # ERROR HANDLING: If anything else goes wrong, log it and send an error event
        # This ensures the client always gets a response, even on failure
        logger.error(f"Error processing agent stream: {e}", exc_info=True)
        yield _create_event("error", {"error": str(e)})
